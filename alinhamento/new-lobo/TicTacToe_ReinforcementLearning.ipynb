{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TicTacToe_ReinforcementLearning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xssjTj8-xzt5",
        "colab_type": "text"
      },
      "source": [
        "# Reinforcement Learning\n",
        "\n",
        "**What is Reinforcement Learning**\n",
        "\n",
        "Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize some notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning. \n",
        "\n",
        "**Objective**\n",
        "\n",
        "Basically, we will implement a simple game  (tic-tac-toe) to enhance the learning of the reinfocement learning algorithm.\n",
        "\n",
        "**How we will implement**\n",
        "\n",
        "To formulate this reinfocement learning problem, we will focus int three major components: **state, action and reward**.\n",
        "\n",
        "Therefore, we will initialise a 3x3 board with zeros indicationg available positions and update positions with 1 if player 1 takes a move and -1 if player 2 takes a move. The action is what positions a  player can choose based on the current board state. Reward is between 0 and 1 and is only given at the end of the game.\n",
        "\n",
        "For each state of the game, we will get the hash code of the current state of the board. To implement this, we'll use hash function, that is, we'll map the current board to a hash code to specify each board with its own code.\n",
        "\n",
        "**Reference**\n",
        "\n",
        "https://towardsdatascience.com/reinforcement-learning-implement-tictactoe-189582bea542\n",
        "\n",
        "https://en.wikipedia.org/wiki/Reinforcement_learning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azpWn47mtcQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will follow this two sessions to implement an algorithm to\n",
        "# make a machine learn to play tic-tac-toe through the reinforcement learning\n",
        "# algorithm:\n",
        "#\n",
        "#   1) Train two agents (machine) to play against each other and save their\n",
        "#   policy that was given by the reward at the end of each game \n",
        "#   2) Load the policy and make the agent to play against human\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Rri0ix8T5ez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # import numpy to use a simple array to represent the board\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bCBSaTvT_dD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tic-Tac-Toe board rows and columns\n",
        "BOARD_ROWS = 3\n",
        "BOARD_COLS = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPUIWMVQUDNz",
        "colab_type": "text"
      },
      "source": [
        "### Board State\n",
        "\n",
        "\n",
        "\n",
        "1.   Act as both board and judger\n",
        "2.   It has functions recording board state of both players and update state when either player takes an action\n",
        "3. At the end of the game, its be able to judge what was the best play and give reward to the player that did it  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao2T82D2vc4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class State:\n",
        "  \n",
        "  # Its like cpp constructor but here we have a predefined method (__init__)\n",
        "  def __init__(self, p1, p2):\n",
        "    # Init the board with zeros (its a 3x3 array)\n",
        "    self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
        "    self.p1 = p1\n",
        "    self.p2 = p2\n",
        "    self.isEnd = False\n",
        "    self.boardHash = None # hash of the board is not initialized because we will use getHash function to do it\n",
        "    #init p1 plays first\n",
        "    self.playerSymbol = 1\n",
        "\n",
        "  # Hashes the current board states, so that ir can be stored in \n",
        "  # the state-value dictionary\n",
        "  def getHash(self):\n",
        "    # reshape the 3x3 array to a 1x9 vector (to get the board hash)\n",
        "    self.boardHash = str(self.board.reshape(BOARD_COLS * BOARD_ROWS))\n",
        "    return self.boardHash\n",
        "\n",
        "  # Checks sum of rows, columns and diagonals, and return 1 if p1 \n",
        "  # wins, -1 if p2 wins, 0 if draw and None if the game is not yed ended\n",
        "  # At the end of the game, 1 is rewarded to winner and 0 to loser. \n",
        "  # We can possibly modify the reward according to the game\n",
        "  def winner(self):\n",
        "    # row\n",
        "    for i in range(BOARD_ROWS):\n",
        "      # checks if has a row with p1\n",
        "      if sum(self.board[i, :]) == 3:\n",
        "        self.isEnd = True\n",
        "        return 1\n",
        "      # checks if has a row with p2\n",
        "      if sum(self.board[i, :]) == -3:\n",
        "        self.isEnd = True\n",
        "        return -1\n",
        "    \n",
        "    # col\n",
        "    for i in range(BOARD_COLS):\n",
        "      # check if has a col with p1\n",
        "      if sum(self.board[:, i]) == 3:\n",
        "        self.isEnd = True\n",
        "        return 1\n",
        "      # checks if has a col with p2\n",
        "      if sum(self.board[:, i]) == -3:\n",
        "        self.isEnd = True\n",
        "        return -1\n",
        "\n",
        "    #diagonal\n",
        "    diag_sum1 = sum([self.board[i, i] for i in range(BOARD_COLS)])\n",
        "    diag_sum2 = sum([self.board[i, BOARD_COLS - i - 1] for i in range(BOARD_COLS)])\n",
        "    diag_sum = max(abs(diag_sum1), abs(diag_sum2))\n",
        "\n",
        "    # If have a triple of player 1 (or 2) in the diagonal\n",
        "    if diag_sum == 3:\n",
        "      self.isEnd = True\n",
        "      # If the player 1 won\n",
        "      if diag_sum1 == 3 or diag_sum2 == 3:\n",
        "        return 1\n",
        "      # If the player 2 won\n",
        "      else:\n",
        "        return -1\n",
        "\n",
        "    # tie\n",
        "    # no available positions\n",
        "    if len(self.availablePositions()) == 0:\n",
        "      self.isEnd = True\n",
        "      return 0\n",
        "\n",
        "    # not end\n",
        "    self.isEnd = False\n",
        "    return None\n",
        " \n",
        "  # Checks for possible positions to play\n",
        "  # Returns a tuple to be used in the updateState method\n",
        "  def availablePositions(self):\n",
        "    positions = [] # initialize a list of tuples (ordered pairs)\n",
        "    for i in range(BOARD_ROWS):\n",
        "      for j in range(BOARD_COLS):\n",
        "        # checks if its a available position\n",
        "        if self.board[i, j] == 0:\n",
        "          positions.append((i, j)) # need to be a tuple\n",
        "    return positions\n",
        "\n",
        "  # Update the state of the board according to the players move\n",
        "  def updateState(self, position):\n",
        "    self.board[position] = self.playerSymbol\n",
        "    # switch to another player\n",
        "    self.playerSymbol = -1 if self.playerSymbol == 1 else 1\n",
        "\n",
        "  # only when the game ends\n",
        "  def giveReward(self):\n",
        "    result = self.winner()\n",
        "    # backpropagate reward\n",
        "    # checks if the p1 won\n",
        "    if result == 1:\n",
        "      self.p1.feedReward(1)\n",
        "      self.p2.feedReward(0)\n",
        "    # checks ithe p2 won\n",
        "    elif result == -1:\n",
        "      self.p1.feedReward(0)\n",
        "      self.p2.feedReward(1)\n",
        "    else:\n",
        "      # if tie\n",
        "      self.p1.feedReward(0.1)\n",
        "      self.p2.feedReward(0.5)\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
        "    self.boardHash = None\n",
        "    self.isEnd = False\n",
        "    self.playerSymbol = 1\n",
        "\n",
        "\n",
        "\n",
        "  # Training the algorithm:\n",
        "  # During training, the process for each player is:\n",
        "  #   - Look for available positions\n",
        "  #   - Choose action\n",
        "  #   - Update board state and add the action to players states\n",
        "  #   - Judge if reach the end of the game and give reward accordingly\n",
        "  def play(self, rounds=100):\n",
        "    for i in range(rounds):\n",
        "      if i % 1000 == 0:\n",
        "        print(\"Rounds {}\".format(i))\n",
        "      while not self.isEnd:\n",
        "        # Player 1\n",
        "        positions = self.availablePositions()\n",
        "        p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
        "        # take action and update board state\n",
        "        self.updateState(p1_action)\n",
        "        board_hash = self.getHash()\n",
        "        self.p1.addState(board_hash)\n",
        "        # check board status if is end\n",
        "\n",
        "        win = self.winner()\n",
        "        if win is not None:\n",
        "          #self.showBoard()\n",
        "          #ended with p1 either win or draw\n",
        "          self.giveReward()\n",
        "          self.p1.reset()\n",
        "          self.p2.reset()\n",
        "          self.reset()\n",
        "          break\n",
        "\n",
        "        else:\n",
        "          #Player 2\n",
        "          positions = self.availablePositions()\n",
        "          p2_action = self.p2.chooseAction(positions, self.board, self.playerSymbol)\n",
        "          self.updateState(p2_action)\n",
        "          board_hash = self.getHash()\n",
        "          self.p2.addState(board_hash)\n",
        "\n",
        "          win = self.winner()\n",
        "          if win is not None:\n",
        "            #self.showBoard()\n",
        "            #ended with p2 either win or draw\n",
        "            self.giveReward()\n",
        "            self.p1.reset()\n",
        "            self.p2.reset() \n",
        "            self.reset()\n",
        "            break\n",
        "\n",
        "  # play against human\n",
        "  def play2(self):\n",
        "    while not self.isEnd:\n",
        "      # Player 1\n",
        "      positions = self.availablePositions()\n",
        "      p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
        "      # take action and upate board state\n",
        "      self.updateState(p1_action)\n",
        "      self.showBoard()\n",
        "      # check board status if it is end\n",
        "      win = self.winner()\n",
        "      if win is not None:\n",
        "        if win == 1:\n",
        "          print(self.p1.name, \"wins!\")\n",
        "        else:\n",
        "          print(\"tie!\")\n",
        "        self.reset()\n",
        "        break\n",
        "\n",
        "      else:\n",
        "        # Player 2\n",
        "        positions = self.availablePositions()\n",
        "        p2_action = self.p2.chooseAction(positions)\n",
        "\n",
        "        self.updateState(p2_action)\n",
        "        self.showBoard()\n",
        "        win = self.winner()\n",
        "        if win is not None:\n",
        "          if win == -1:\n",
        "            print(self.p2.name, \"wins!\")\n",
        "          else:\n",
        "            print(\"tie!\")\n",
        "          self.reset()\n",
        "          break\n",
        "\n",
        "\n",
        "  def showBoard(self):\n",
        "    # p1: x  p2: o\n",
        "    for i in range(0, BOARD_ROWS):\n",
        "      print('-------------')\n",
        "      out = '| '\n",
        "      for j in range(0, BOARD_COLS):\n",
        "        if self.board[i, j] == 1:\n",
        "          token = 'x'\n",
        "        if self.board[i, j] == -1:\n",
        "          token = 'o'\n",
        "        if self.board[i, j] == 0:\n",
        "          token = ' '\n",
        "        out += token + ' | '\n",
        "      print(out)\n",
        "    print('-------------')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7K5B01lXT4l",
        "colab_type": "text"
      },
      "source": [
        "### Player\n",
        "\n",
        "* Represent our agent\n",
        "* The player is able to:\n",
        "  1.   Choose actions base on current estimation of the states \n",
        "  2. Record all states of the game\n",
        "  3. Update states-value estimation after each game\n",
        "  4. Save and load the policy \n",
        "* We keep track of all positions the player's been during each game in a list and update the correspondig states in a dictonary\n",
        "* We use gama-greedy method to balance between exploration and exploitation     \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMSqYt9CWgKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Player:\n",
        "  # init a dict storing state-value pair and update\n",
        "  # the estimates at the end of each game\n",
        "\n",
        "  # exp_rate means gamma = 0.3, in other words 70% of the\n",
        "  # time our agent will take greedy action, witch is choosing\n",
        "  # action based on current estimation of states-value, and 30%\n",
        "  # of the time our agent will take random action\n",
        "  def __init__(self, name, exp_rate=0.3):\n",
        "    self.name = name\n",
        "    self.states = [] # record all positions taken\n",
        "    self.lr = 0.2\n",
        "    self.exp_rate = exp_rate\n",
        "    self.decay_gamma = 0.9\n",
        "    self.states_value = {} # state -> value\n",
        "\n",
        "  def getHash(self, board):\n",
        "    boardHash = str(board.reshape(BOARD_COLS * BOARD_ROWS))\n",
        "    return boardHash\n",
        "\n",
        "  def chooseAction(self, positions, current_board, symbol):\n",
        "    # generates a random number and compares it with exp_rate\n",
        "    if np.random.uniform(0, 1) <= self.exp_rate:\n",
        "      # take random action according to the positions\n",
        "      idx = np.random.choice(len(positions))\n",
        "      action = positions[idx]\n",
        "    else:\n",
        "      value_max = -999\n",
        "      for p in positions:\n",
        "        next_board = current_board.copy()\n",
        "        next_board[p] = symbol\n",
        "        next_boardHash = self.getHash(next_board)\n",
        "        # we store the hash of the board into state-value dict,\n",
        "        # and while axploitation, we hash de next board state and\n",
        "        # choose the action that returns the maximum value of the next state\n",
        "        # if the get method (dict) didnt found the especific value, returns None\n",
        "        value = 0 if self.states_value.get(next_boardHash) is None else self.states_value.get(next_boardHash)\n",
        "        # print(\"value\", value)\n",
        "        if value >= value_max:\n",
        "          value_max = value\n",
        "          action = p\n",
        "    # print(\"{} takes action {}\".format(self.name, action))\n",
        "    return action\n",
        "\n",
        "  # append hash state\n",
        "  def addState(self, state):\n",
        "    self.states.append(state)\n",
        "\n",
        "  # At the end of the game, backpropagate and update states-value\n",
        "  # To update states-value we will apply the following formula:\n",
        "  # V(St) <-> V(St) + x [V(St+1) - V(St)]\n",
        "  # that tells us that the value of state t equals the value of\n",
        "  # state t adding the difference between the value of the state\n",
        "  # t+1 and the value o the state t, which is multiplied by a \n",
        "  # learning rate x (Given the reward of intermediate state is 0)\n",
        "  # We will update teh current value based on our latest observation\n",
        "  def feedReward(self, reward):\n",
        "    # the positions of each game is sotred in self.states and when\n",
        "    # the agent reach the end of the game, the estimates are updates\n",
        "    # in reversed fashion\n",
        "    for st in reversed(self.states):\n",
        "      # if st doesnt exists\n",
        "      if self.states_value.get(st) is None:\n",
        "        self.states_value[st] = 0\n",
        "      self.states_value[st] += self.lr * (self.decay_gamma * reward - self.states_value[st])\n",
        "      reward = self.states_value[st]\n",
        "\n",
        "\n",
        "  # ??????????\n",
        "  def reset(self):\n",
        "    self.states = []\n",
        "\n",
        "  # Save policy to play with a human player\n",
        "  # this dunction is called at the end of the training\n",
        "  def savePolicy(self):\n",
        "    fw = open('policy_' + str(self.name), 'wb')\n",
        "    pickle.dump(self.states_value, fw)\n",
        "    fw.close()\n",
        "\n",
        "  def loadPolicy(self, file):\n",
        "    fr = open(file, 'rb')\n",
        "    self.states_value = pickle.load(fr)\n",
        "    fr.close()\n",
        "\n",
        "  # ?????????\n",
        "  \n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEl1xfhQY7Hu",
        "colab_type": "text"
      },
      "source": [
        "### Human Player\n",
        "\n",
        "\n",
        "\n",
        "*   Represents the player effectively\n",
        "*   This class includes only 1 usable function chooseAction which requires us to input the board position we hope to take. And we also need to modify a bit on the play function inside State class \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlTb9mu-aPTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HumanPlayer:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "\n",
        "  def chooseAction(self, positions):\n",
        "    while True:\n",
        "      row = int(input(\"Input your action row: \"))\n",
        "      col = int(input(\"Input your action col: \"))\n",
        "      action = (row, col)\n",
        "      if action in positions:\n",
        "        return action\n",
        "\n",
        "  # append a hash state\n",
        "  def addState(self, state):\n",
        "    pass\n",
        "\n",
        "  # at the end of the game, backpropagate and update states value\n",
        "  def feedReward(self, reward):\n",
        "    pass\n",
        "\n",
        "  def reset(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3EUUma6Zbt0",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8kgsvitUij7",
        "colab_type": "code",
        "outputId": "ec639f7e-6ecb-4778-f6fa-049744060ae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "# Training the algorithm\n",
        "p1 = Player(\"p1\")\n",
        "p2 = Player(\"p2\")\n",
        "\n",
        "st = State(p1, p2)\n",
        "print(\"training...\")\n",
        "st.play(10000)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training...\n",
            "Rounds 0\n",
            "Rounds 1000\n",
            "Rounds 2000\n",
            "Rounds 3000\n",
            "Rounds 4000\n",
            "Rounds 5000\n",
            "Rounds 6000\n",
            "Rounds 7000\n",
            "Rounds 8000\n",
            "Rounds 9000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exkpt4L_TnpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p1.savePolicy()\n",
        "p2.savePolicy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1Qsxp7aTohA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p1.loadPolicy(\"policy_p1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK5YCfkBZfsy",
        "colab_type": "text"
      },
      "source": [
        "###Human vs Computer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1iMbJzoTq1G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "outputId": "191142b3-6ed5-4b90-bd9d-e9c354e34a33"
      },
      "source": [
        "p1 = Player(\"computer\", exp_rate=0)\n",
        "p1.loadPolicy(\"policy_p1\")\n",
        "\n",
        "p2 = HumanPlayer(\"human\")\n",
        "\n",
        "st = State(p1, p2)\n",
        "st.play2()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "| x |   |   | \n",
            "-------------\n",
            "Input your action row: 0\n",
            "Input your action col: 2\n",
            "-------------\n",
            "|   |   | o | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "| x |   |   | \n",
            "-------------\n",
            "-------------\n",
            "|   |   | o | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "| x |   | x | \n",
            "-------------\n",
            "Input your action row: 2\n",
            "Input your action col: 1\n",
            "-------------\n",
            "|   |   | o | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "| x | o | x | \n",
            "-------------\n",
            "-------------\n",
            "| x |   | o | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "| x | o | x | \n",
            "-------------\n",
            "Input your action row: 1\n",
            "Input your action col: 1\n",
            "-------------\n",
            "| x |   | o | \n",
            "-------------\n",
            "|   | o |   | \n",
            "-------------\n",
            "| x | o | x | \n",
            "-------------\n",
            "-------------\n",
            "| x |   | o | \n",
            "-------------\n",
            "| x | o |   | \n",
            "-------------\n",
            "| x | o | x | \n",
            "-------------\n",
            "computer wins!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}