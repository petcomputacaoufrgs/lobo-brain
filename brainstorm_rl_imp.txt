

INTRODUÇÃO AO PENSAMENTO
Já estudado os conceitos basicos de rl e q-learning, podemos começar a pensar 
formas de adaptarmos nosso projeto a funcionar a partir desses métodos.
Dessa forma, temos que levar em consideração alguns pontos estudados:
1. Conceitos básicos de Reinforcement learning
2. Conceitos básicos da equação de Markov e MDP
3. Conceitos básicos de métodos de cálculo da política ótima de uma ação 
de determinado processo
4. Como iremos definir a 5-tupla (S, A, P, R, G) de acordo com o nosso projeto, onde:
    - S é um conjunto de estados (tabuleiros)
    - A conjunto de ações (jogadas/movimentos)
    - P(s, a, s') é probabilidade da ação a no estado s no tempo t levar o estado s
    ao estado s' no tempo t+1 (definido a partir da equação de Markov)
    - R(s, a ,s') é a recompensa imediata recebida da transição do estado s ao s' 
    através da ação a (evaluate, no final de cada jogo)
    - G é o fator de desconto que é usado para gerar uma recompensa com desconto
     (definido a partir do MDP)

MÉTODO DE IMPLEMENTAÇÃO

Para reformularmos esse problema para o método de Reinforcement Learning, iremos
focar em tres componentes majoritarios: estado, ação e recompensa.
Nós iremos ter um tabuleiro 3x3 indicando cada posição, por exemplo, 1 se o jogador um
ocupa a posição, 2 se o jogador dois e 0 se a posição esta disponivel.
Cada jogada pode ser baseada a partir de uma posiçaõ disponivel no tabuleiro, oque nossa
função move ja faz bem. A recompensa pode ser determinada entre 0 e 1 e só é dada no fim
do jogo.
Para cada estado de jogo nos vamos pegar o código hash do estado atual do tabuleiro 
(função hash que ja temos) utilizando a funçao hash que irá mapear cada estado de tabuleiro
para um código especifico para aquele estado.

Logo, iremos seguir duas seções para implementar o algoritmo:
    1. Treinar dois agentes jogando entre eles mesmos e salvar sua política que
    foi estimada a partir da recompensa dada no final de cada jogo
    2. Carregar a politica e fazer com que o agente jogue contra humano

Assim, teremos algumas classes que iremos definir adiante. Entre elas teremos State class,
Player class e HumanPlayer class


CLASSE STATE
Representa o estado atual do tabuleiro. Vai agir tanto quanto tabuleiro e juiz a fim de
determinar a recompensa.
Nela contem funcoes que guardam o estado do jogo, a atualização do estado e a movimentação.
NO final do jogo, o juiz ira determinar qual foi a melhor jogada e  dar uma recompensa para
o jogador que a fez.
Protótipo da classe:

OBS: /*****/ indica que aqui vai dar merda pra fazer em cpp

class State
{
    private:
		vector<vector<char>> board;
		unsigned int boardHash;

    public:
        char p1;
        char p2;
        bool isEnd;
        char playerSymbol;
        State();
        State(vector<vector<char>> board, p1, p2);
    
        vector<vector<char>> getBoard();
		void setBoard(vector<vector<char>>);
		void setPosition(int i, int j, char player);

		unsigned int getHash();
		void print();
        char winner();//verifica se o jogo houve vencedor, empatou, ou nao terminou
        vector<vector<int>> availablePositions();//verifica as possiveis posicoes para jogar. retorna um vetor de tuplas
        void updateState(vector<int>> position);//atualiza o estado do jogo, se o jogador 1 ou 2 joga, atualizando a movimentação
        void giveReward();//da a recompensa no final de cada jogo, atraves do metodo feedReward
        void reset();//reseta todos atributos da classe State

       /*****/ void play_train(rounds); //treina o algoritmo
       /*****/void play_human();//joga contra o humano
}




CLASSE PLAYER
Representa o jogador (maquina) que irá ser treinado.
O jogador pode realizar as seguintes açoes:
    1. Escolher ações baseadas a partir da atual estimação dos estados
    2. Guardar todos os estados do jogo
    3. Atualizar a estimação estado-valor no final de cada jogo
    4. Salvar e carregar a política
Nós iremos utilizar o método MDP para encontrar a melhor ação usando o fator de 
balanceamento gama.

Protótipo da classe:
OBS: /*****/ indica que aqui vai dar merda pra fazer em cpp
class Player
{
    public:
        char name[]; //nao precisa
        vector<vector<int>> states;//guarda todas as posicoes jogadas
        float lr; //nao entendi exatamente para que serve mas aparentemente é para calcular a politica
        float exp_rate; //é o gama do MDP
        float decay_gamma;
        std::map<float, <vector<int>>; //  é o dicionario em cpp segue o uso abaixo:

        /*
        para gerar a chave estado-valor
        valueForStates[2.28] = {1, 2}

        podemos retirar o numero de chaves que ligaa valor com
        std::map::count()

        podemos um iterador que encontrou o objeto que queriemos com
        std::map::find()

        podemos iterar no dicionario:

        std::map<int, std::string> values;
        // ... 
        for(std::map<int, std::string>::value_type& x : values)
        {
            std::cout << x.first << "," << x.second << std::endl;
        }

        podemos apagar uma chave-valor com a função erase() que recebe a chave ou o valor que queremos apagar

        */

        /****/ vector<int> chooseAction(vector<vector<int>> positions, vector<vector<char>> current_board, char playerSymbol);

        void addState(vector<int> state);

        /****/ void feedReward(reward);

        void reset();
        /****/ savePolicy();//salva a politica atraves de MDP
        /****/ loadPolicy(file);// carrega a politica

}

CLASSE HUMANPLAYER
Representa o jogador efetivamente. Esra classe inclui somente uma função
usável que vai pegar a jogada do jogador, ou seja, a posicao do tabuleiro que
ele vai jogar.

Prototipo da classe:

class HumanPlayer
{

    public:
        char name[]; //nao é necessario

        vector<vector<int>> chooseAction; //pega a posicao do tabuleiro (tupla) que o jogador quer jogar
}

